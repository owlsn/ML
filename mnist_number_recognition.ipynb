{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    mnist手写数字识别\n",
    "    主要是为了熟悉tensorflow的运行过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist.input_data import read_data_sets\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_DATA/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_DATA/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_DATA/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_DATA/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#载入mnist数据\n",
    "input_data = read_data_sets(\"MNIST_DATA/\", False)\n",
    "batch_size = 100\n",
    "image_size = 28\n",
    "hidden_dim1 = 100\n",
    "hidden_dim2 = 200\n",
    "image_pixels = image_size * image_size\n",
    "out_nums = 10\n",
    "learning_rate = 0.005\n",
    "max_steps = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 2.30354642868042 (0.2356429100036621 sec)\n",
      "Step 100: loss = 2.2758095264434814 (0.002650022506713867 sec)\n",
      "Step 200: loss = 2.219822883605957 (0.0027740001678466797 sec)\n",
      "Step 300: loss = 2.1608426570892334 (0.002747058868408203 sec)\n",
      "Step 400: loss = 2.0400402545928955 (0.003114938735961914 sec)\n",
      "Step 500: loss = 1.9531081914901733 (0.0026700496673583984 sec)\n",
      "Step 600: loss = 1.8012624979019165 (0.002621173858642578 sec)\n",
      "Step 700: loss = 1.6865527629852295 (0.0027341842651367188 sec)\n",
      "Step 800: loss = 1.387156367301941 (0.0027730464935302734 sec)\n",
      "Step 900: loss = 1.257866621017456 (0.002726316452026367 sec)\n",
      "Num examples: 5500000  Num correct: 4352065  Precision @ 1: 0.7913\n",
      "Num examples: 500000  Num correct: 395708  Precision @ 1: 0.7914\n",
      "Num examples: 1000000  Num correct: 791289  Precision @ 1: 0.7913\n",
      "Step 1000: loss = 1.0945743322372437 (0.011349916458129883 sec)\n",
      "Step 1100: loss = 0.8651325106620789 (0.0027430057525634766 sec)\n",
      "Step 1200: loss = 0.7994837164878845 (0.0026891231536865234 sec)\n",
      "Step 1300: loss = 0.8189288973808289 (0.0027730464935302734 sec)\n",
      "Step 1400: loss = 0.7777454853057861 (0.0027649402618408203 sec)\n",
      "Step 1500: loss = 0.7886578440666199 (0.09502387046813965 sec)\n",
      "Step 1600: loss = 0.7074301242828369 (0.002753019332885742 sec)\n",
      "Step 1700: loss = 0.6958401203155518 (0.0027871131896972656 sec)\n",
      "Step 1800: loss = 0.5827304720878601 (0.002698183059692383 sec)\n",
      "Step 1900: loss = 0.547147810459137 (0.0027828216552734375 sec)\n",
      "Num examples: 5500000  Num correct: 4762383  Precision @ 1: 0.8659\n",
      "Num examples: 500000  Num correct: 432931  Precision @ 1: 0.8659\n",
      "Num examples: 1000000  Num correct: 865964  Precision @ 1: 0.8660\n",
      "Step 2000: loss = 0.39053359627723694 (0.010692834854125977 sec)\n",
      "Step 2100: loss = 0.4439743161201477 (0.0027379989624023438 sec)\n",
      "Step 2200: loss = 0.3934803009033203 (0.0026412010192871094 sec)\n",
      "Step 2300: loss = 0.5378629565238953 (0.002644062042236328 sec)\n",
      "Step 2400: loss = 0.4796016812324524 (0.0026388168334960938 sec)\n",
      "Step 2500: loss = 0.3523133099079132 (0.0026760101318359375 sec)\n",
      "Step 2600: loss = 0.50339275598526 (0.002733945846557617 sec)\n",
      "Step 2700: loss = 0.4652096629142761 (0.002660036087036133 sec)\n",
      "Step 2800: loss = 0.43375450372695923 (0.0028791427612304688 sec)\n",
      "Step 2900: loss = 0.43189898133277893 (0.003435850143432617 sec)\n",
      "Num examples: 5500000  Num correct: 4878500  Precision @ 1: 0.8870\n",
      "Num examples: 500000  Num correct: 443502  Precision @ 1: 0.8870\n",
      "Num examples: 1000000  Num correct: 886955  Precision @ 1: 0.8870\n",
      "Step 3000: loss = 0.35138604044914246 (0.010609865188598633 sec)\n",
      "Step 3100: loss = 0.3277815282344818 (0.0026559829711914062 sec)\n",
      "Step 3200: loss = 0.6448149085044861 (0.0026488304138183594 sec)\n",
      "Step 3300: loss = 0.36436161398887634 (0.002635955810546875 sec)\n",
      "Step 3400: loss = 0.36269837617874146 (0.09318208694458008 sec)\n",
      "Step 3500: loss = 0.3724438548088074 (0.002741098403930664 sec)\n",
      "Step 3600: loss = 0.36023572087287903 (0.002680063247680664 sec)\n",
      "Step 3700: loss = 0.30595120787620544 (0.002585887908935547 sec)\n",
      "Step 3800: loss = 0.2987949848175049 (0.0025730133056640625 sec)\n",
      "Step 3900: loss = 0.33259502053260803 (0.0027132034301757812 sec)\n",
      "Num examples: 5500000  Num correct: 4932954  Precision @ 1: 0.8969\n",
      "Num examples: 500000  Num correct: 448506  Precision @ 1: 0.8970\n",
      "Num examples: 1000000  Num correct: 896883  Precision @ 1: 0.8969\n"
     ]
    }
   ],
   "source": [
    "#构建训练模型\n",
    "#初始化参数\n",
    "#定义损失函数\n",
    "#定义梯度下降参数\n",
    "#批量训练\n",
    "def do_eval(sess,\n",
    "            eval_correct,\n",
    "            images_placeholder,\n",
    "            labels_placeholder,\n",
    "            data_set):\n",
    "    \"\"\"Runs one evaluation against the full epoch of data.\n",
    "\n",
    "    Args:\n",
    "    sess: The session in which the model has been trained.\n",
    "    eval_correct: The Tensor that returns the number of correct predictions.\n",
    "    images_placeholder: The images placeholder.\n",
    "    labels_placeholder: The labels placeholder.\n",
    "    data_set: The set of images and labels to evaluate, from\n",
    "      input_data.read_data_sets().\n",
    "    \"\"\"\n",
    "    # And run one epoch of eval.\n",
    "    true_count = 0  # Counts the number of correct predictions.\n",
    "    steps_per_epoch = data_set.num_examples # FLAGS.\n",
    "\n",
    "    num_examples = steps_per_epoch * batch_size\n",
    "    for step in range(steps_per_epoch):\n",
    "        images_pl, labels_pl = input_data.train.next_batch(batch_size)\n",
    "        feed_dict = {images_placeholder : images_pl, labels_placeholder : labels_pl}\n",
    "        true_count += sess.run(eval_correct, feed_dict=feed_dict)\n",
    "    precision = float(true_count) / num_examples\n",
    "    print('Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' %(num_examples, true_count, precision))\n",
    "\n",
    "#初始化要训练的图片和标签数据的框架，\n",
    "#输入数据为batch_size*image_pixels的矩阵\n",
    "#输出的labels为batch_size的向量\n",
    "with tf.Graph().as_default():\n",
    "    images_placeholder = tf.compat.v1.placeholder(tf.float32, (batch_size, image_pixels))\n",
    "    labels_placeholder = tf.compat.v1.placeholder(tf.int32, (batch_size))\n",
    "\n",
    "    #搭建两层的卷积神经网络\n",
    "    #第一层的参数设置\n",
    "    with tf.compat.v1.name_scope(\"hidden1\"):\n",
    "        w1 = tf.Variable(tf.random.truncated_normal(shape=(image_pixels, hidden_dim1),\n",
    "                                                    stddev=(1.0/math.sqrt(float(image_pixels)))), name=\"weights\")\n",
    "        b1 = tf.Variable(tf.zeros([hidden_dim1]), name=\"bias\")\n",
    "        hidden1 = tf.nn.relu(tf.matmul(images_placeholder, w1) + b1)\n",
    "        \n",
    "    with tf.compat.v1.name_scope(\"hidden2\"):\n",
    "        w2 = tf.Variable(tf.random.truncated_normal(shape=(hidden_dim1, hidden_dim2),\n",
    "                                                    stddev=(1.0/math.sqrt(float(hidden_dim2)))), name=\"weights\")\n",
    "        b2 = tf.Variable(tf.zeros([hidden_dim2]), name=\"bias\")\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, w2) + b2)\n",
    "        \n",
    "    #然后最后连接一层softmax层\n",
    "    with tf.compat.v1.name_scope(\"softmax\"):\n",
    "        ws = tf.Variable(tf.random.truncated_normal(shape=[hidden_dim2, out_nums],\n",
    "                                                    stddev=(1.0/math.sqrt(float(hidden_dim2)))), name=\"weights\")\n",
    "        bs = tf.Variable(tf.zeros([out_nums]), name=\"bias\")\n",
    "        logit = tf.matmul(hidden2, ws) + bs\n",
    "        \n",
    "    #得到卷积层的输出后计算损失函数\n",
    "    loss = tf.compat.v1.losses.sparse_softmax_cross_entropy(logits=logit, labels=tf.cast(labels_placeholder, tf.int32))\n",
    "    \n",
    "    #上面的这些过程可以看作是前向计算损失\n",
    "    #接下来的过程就是反向传播训练参数\n",
    "    tf.compat.v1.summary.scalar(\"loss\", loss)\n",
    "    optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate)\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    \n",
    "    #得到的train_op就是训练出来的结果，然后用结果计算准确度\n",
    "    correct = tf.nn.in_top_k(predictions=logit, targets=tf.cast(labels_placeholder, tf.int64), k=1)\n",
    "    eval_correct = tf.reduce_sum(input_tensor=tf.cast(correct, tf.int32))\n",
    "    \n",
    "    summary = tf.compat.v1.summary.merge_all()\n",
    "    \n",
    "    #tensorflow上述只是定义了运算过程，还没有将具体的数据放在运算过程中执行，\n",
    "    #接下来就是具体的运算\n",
    "    #初始化所有的变量\n",
    "    init = tf.compat.v1.global_variables_initializer()\n",
    "    \n",
    "    saver = tf.compat.v1.train.Saver()\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        summary_writer = tf.compat.v1.summary.FileWriter(\"logs/\", sess.graph)\n",
    "        sess.run(init)\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            #这里是将实际的数据填入上面定义好的运算过程中\n",
    "            images_feed, labels_feed = input_data.train.next_batch(batch_size)\n",
    "            \n",
    "            feed_dict = {images_placeholder : images_feed, labels_placeholder : labels_feed}\n",
    "            \n",
    "            _, loss_value = sess.run([train_op, loss], feed_dict=feed_dict)\n",
    "            \n",
    "            duration = time.time() - start_time\n",
    "            if step % 100 == 0:\n",
    "                print('Step {}: loss = {} ({} sec)'.format(step, loss_value, duration))\n",
    "                \n",
    "                summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "                summary_writer.add_summary(summary_str, step)\n",
    "                summary_writer.flush()\n",
    "                \n",
    "            # Save a checkpoint and evaluate the model periodically.\n",
    "            if (step + 1) % 1000 == 0 or (step + 1) == max_steps:\n",
    "                checkpoint_file = 'logs/model.ckpt'\n",
    "                saver.save(sess, checkpoint_file, global_step=step)\n",
    "                # Evaluate against the training set.\n",
    "#                 print('Training Data Eval:')\n",
    "                do_eval(sess,\n",
    "                        eval_correct,\n",
    "                        images_placeholder,\n",
    "                        labels_placeholder,\n",
    "                        input_data.train)\n",
    "                # Evaluate against the validation set.\n",
    "#                 print('Validation Data Eval:')\n",
    "                do_eval(sess,\n",
    "                        eval_correct,\n",
    "                        images_placeholder,\n",
    "                        labels_placeholder,\n",
    "                        input_data.validation)\n",
    "                # Evaluate against the test set.\n",
    "#                 print('Test Data Eval:')\n",
    "                do_eval(sess,\n",
    "                        eval_correct,\n",
    "                        images_placeholder,\n",
    "                        labels_placeholder,\n",
    "                        input_data.test)\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
