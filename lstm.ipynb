{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    lstm的python实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    前面生成数据的部分和rnn数据处理部分相同\n",
    "    样本数据来自于reddit评论，使用nltk处理后作为训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vacabulary size 8000\n",
      "The least frequent word in our vocabulary is 'documentary' and appeared times 10\n",
      "[0, 6, 3494, 7, 155, 795, 25, 222, 8, 32, 20, 202, 4954, 350, 91, 6, 66, 207, 5, 2]\n",
      "[6, 3494, 7, 155, 795, 25, 222, 8, 32, 20, 202, 4954, 350, 91, 6, 66, 207, 5, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import itertools\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "#定义一些基本的常量\n",
    "vocabulary_size = 8000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "start_token = \"SENTENCE_START\"\n",
    "end_token = \"SENTENCE_END\"\n",
    "\n",
    "#处理数据，这里的数据是从reddit上面下载的英文的评论，预先使用nltk工具处理后，\n",
    "#得到原始的训练数据集\n",
    "file_path = \"reddit-comments-2015-08.csv\"\n",
    "tokenized = []\n",
    "original = []\n",
    "with open(file_path, 'r') as f:\n",
    "    #读取内容\n",
    "    reader = csv.reader(f, skipinitialspace=True)\n",
    "    for x in reader:\n",
    "        #每个句子前后都加上开始和结束标记，然后再使用nltk分词\n",
    "        sentences = nltk.sent_tokenize(x[0].lower())\n",
    "        for sentence in sentences:\n",
    "            original.append(sentence)\n",
    "            sentence = start_token + \" \" + sentence + \" \" + end_token\n",
    "            tokenized.append(nltk.word_tokenize(sentence))\n",
    "    f.close()\n",
    "\n",
    "#统计词频\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized))\n",
    "\n",
    "#获取常用词，然后构建词频向量\n",
    "vocab = word_freq.most_common(vocabulary_size - 1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([w, i] for i, w in enumerate(index_to_word))\n",
    "\n",
    "print(\"vacabulary size {}\".format(vocabulary_size))\n",
    "print(\"The least frequent word in our vocabulary is '{}' and appeared times {}\".format(vocab[-1][0], vocab[-1][1]))\n",
    "\n",
    "#将所有不在词汇表中的词替换为unknown token\n",
    "for i, sent in enumerate(tokenized):\n",
    "    tokenized[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "    \n",
    "#创建训练数据\n",
    "#训练的主要目的是预测下一个词，所以x的数据是对应y数据的前一个词\n",
    "x_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized])\n",
    "\n",
    "print(x_train[1])\n",
    "print(y_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#先定义一下常用的激活函数\n",
    "def softmax(x):\n",
    "    x = np.array(x)\n",
    "    max_x = np.max(x)\n",
    "    return np.exp(x - max_x) / np.sum(np.exp(x - max_x))\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "def tanh(x):\n",
    "    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化一些参数\n",
    "data_dim = vocabulary_size\n",
    "hidden_dim = 100\n",
    "learning_rate = 0.005\n",
    "def init_wb():\n",
    "    wh = np.random.uniform(-np.sqrt(1.0 / hidden_dim), np.sqrt(1.0 / hidden_dim), (hidden_dim, hidden_dim))\n",
    "    wx = np.random.uniform(-np.sqrt(1.0 / data_dim), np.sqrt(1.0 / data_dim), (hidden_dim, data_dim))\n",
    "    b = np.random.uniform(-np.sqrt(1.0 / data_dim), np.sqrt(1.0 / data_dim), (hidden_dim, 1))\n",
    "    return wh, wx, b\n",
    "\n",
    "#初始化权重向量，根据公式有\n",
    "#1.遗忘门参数,whf,wxf,bf\n",
    "#2.输入门参数,whi,whi,bi,和新信息参数wha,wxa,ba\n",
    "#3.输出门参数,who,wxo,bo\n",
    "whf, wxf, bf = init_wb()\n",
    "whi, wxi, bi = init_wb()\n",
    "wha, wxa, ba = init_wb()\n",
    "who, wxo, bo = init_wb()\n",
    "\n",
    "#输出权重和偏置，y^ = wy*o+wb\n",
    "wy = np.random.uniform(-np.sqrt(1.0 / hidden_dim), np.sqrt(1.0 / hidden_dim), (data_dim, hidden_dim))\n",
    "by = np.random.uniform(-np.sqrt(1.0 / hidden_dim), np.sqrt(1.0 / hidden_dim), (data_dim, 1))\n",
    "\n",
    "#将这些参数封装在对象中\n",
    "variables = {\n",
    "    \"whf\" : whf, \"wxf\" : wxf, \"bf\" : bf, \"whi\" : whi, \"wxi\" : wxi, \"bi\" : bi,\n",
    "    \"wha\" : wha, \"wxa\" : wxa, \"ba\" : ba,\n",
    "    \"who\" : who, \"wxo\" : wxo, \"bo\" : bo,\n",
    "    \"wy\" : wy, \"by\" : by\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#初始化一些状态向量\n",
    "def init_s(T):\n",
    "    #输入门it,新信息ast，遗忘门ft，输出门ot,ht,输出ys,新细胞状态ct\n",
    "    ist = np.array([np.zeros((hidden_dim, 1))] * (T + 1))\n",
    "    cst = np.array([np.zeros((hidden_dim, 1))] * (T + 1))\n",
    "    fst = np.array([np.zeros((hidden_dim, 1))] * (T + 1))\n",
    "    ost = np.array([np.zeros((hidden_dim, 1))] * (T + 1))                                   \n",
    "    hst = np.array([np.zeros((hidden_dim, 1))] * (T + 1))\n",
    "    yst = np.array([np.zeros((data_dim, 1))] * T)\n",
    "    ast = np.array([np.zeros((hidden_dim, 1))] * (T + 1)) \n",
    "    return {\"ist\" : ist, \"cst\" : cst, \"fst\":fst, \"ost\":ost, \"hst\":hst, \"yst\":yst, \"ast\":ast}\n",
    "\n",
    "#前向传播根据x，计算各种状态向量的值\n",
    "def forward(x):\n",
    "    T = len(x)\n",
    "    status = init_s(T)\n",
    "#     print(\"data_dim:{},hidden_dim:{}\".format(data_dim, hidden_dim))\n",
    "    for t in range(T):\n",
    "        hst_pre = np.array(status[\"hst\"][t - 1]).reshape(-1, 1)\n",
    "        \n",
    "        #ist\n",
    "        status[\"ist\"][t] = cal_gate(variables['whi'], variables['wxi'], variables['bi'], x[t], hst_pre, sigmoid)\n",
    "        #fst\n",
    "        status[\"fst\"][t] = cal_gate(variables['whf'], variables['wxf'], variables['bf'], x[t], hst_pre, sigmoid)\n",
    "        #ast\n",
    "        status[\"ast\"][t] = cal_gate(variables['wha'], variables['wxa'], variables['ba'], x[t], hst_pre, tanh)\n",
    "        #ost\n",
    "        status[\"ost\"][t] = cal_gate(variables['who'], variables['wxo'], variables['bo'], x[t], hst_pre, sigmoid)\n",
    "        #cst\n",
    "        status[\"cst\"][t] = status[\"fst\"][t] * status[\"cst\"][t-1] +  status[\"ist\"][t] * status[\"ast\"][t]\n",
    "        #hst\n",
    "        status[\"hst\"][t] = status[\"ost\"][t] * tanh(status[\"cst\"][t])\n",
    "        #yst\n",
    "        status[\"yst\"][t] = softmax(variables['wy'].dot(status[\"hst\"][t]) + variables['by'])\n",
    "        \n",
    "        \n",
    "    return status\n",
    "        \n",
    "def cal_gate(wh, wx, b, x, hst_pre, activation):\n",
    "    return activation(wh.dot(hst_pre) + wx[:, x].reshape(-1, 1) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#预测函数值\n",
    "def predict(x):\n",
    "    status = forward(x)\n",
    "    pre_y = np.argmax(status[\"yst\"].reshape(len(x), -1), axis=1)\n",
    "    return pre_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2456, 2456])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#测试预测函数\n",
    "predict(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#计算损失,定义为softmax输出函数,损失为交叉熵损失\n",
    "# 计算损失， softmax交叉熵损失函数， (x,y)为多个样本\n",
    "def loss(x, y):\n",
    "    cost = 0        \n",
    "    for i in range(len(y)):\n",
    "        status = forward(x[i])\n",
    "        # 取出 y[i] 中每一时刻对应的预测值\n",
    "        pre_yi = status['yst'][range(len(y[i])), y[i]]\n",
    "        cost -= np.sum(np.log(pre_yi))\n",
    "\n",
    "    # 统计所有y中词的个数, 计算平均损失\n",
    "    N = np.sum([len(yi) for yi in y])\n",
    "    ave_loss = cost / N\n",
    "\n",
    "    return ave_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random loss:8.996643532327656\n"
     ]
    }
   ],
   "source": [
    "print(\"random loss:{}\".format(loss(x_train[:10], y_train[:10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    反向传播计算梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 初始化偏导数 dwh, dwx, db\n",
    "def init_wb_grad():\n",
    "    dwh = np.zeros(variables['whi'].shape)\n",
    "    dwx = np.zeros(variables['wxi'].shape)\n",
    "    db = np.zeros(variables['bi'].shape)\n",
    "\n",
    "    return dwh, dwx, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#反向传播计算梯度\n",
    "def bptt(x, y):\n",
    "    dwhi, dwxi, dbi = init_wb_grad()\n",
    "    dwhf, dwxf, dbf = init_wb_grad()                           \n",
    "    dwho, dwxo, dbo = init_wb_grad()\n",
    "    dwha, dwxa, dba = init_wb_grad()\n",
    "    dwy, dby = np.zeros(variables['wy'].shape), np.zeros(variables['by'].shape)\n",
    "\n",
    "    # 初始化 delta_ct，因为后向传播过程中，此值需要累加\n",
    "    delta_ct = np.zeros((hidden_dim, 1))\n",
    "\n",
    "    # 前向计算\n",
    "    stats = forward(x)\n",
    "    # 目标函数对输出 y 的偏导数\n",
    "    delta_o = stats['yst']\n",
    "    delta_o[np.arange(len(y)), y] -= 1\n",
    "\n",
    "    for t in np.arange(len(y))[::-1]:\n",
    "        # 输出层wy, by的偏导数，由于所有时刻的输出共享输出权值矩阵，故所有时刻累加\n",
    "        dwy += delta_o[t].dot(stats['hst'][t].reshape(1, -1))  \n",
    "        dby += delta_o[t]\n",
    "\n",
    "        # 目标函数对隐藏状态的偏导数\n",
    "        delta_ht = variables['wy'].T.dot(delta_o[t])\n",
    "\n",
    "        # 各个门及状态单元的偏导数\n",
    "        delta_ot = delta_ht * tanh(stats['cst'][t])\n",
    "        delta_ct += delta_ht * stats['ost'][t] * (1-tanh(stats['cst'][t])**2)\n",
    "        delta_it = delta_ct * stats['ast'][t]\n",
    "        delta_ft = delta_ct * stats['cst'][t-1]\n",
    "        delta_at = delta_ct * stats['ist'][t]\n",
    "\n",
    "        delta_at_net = delta_at * (1-stats['ast'][t]**2)\n",
    "        delta_it_net = delta_it * stats['ist'][t] * (1-stats['ist'][t])\n",
    "        delta_ft_net = delta_ft * stats['fst'][t] * (1-stats['fst'][t])\n",
    "        delta_ot_net = delta_ot * stats['ost'][t] * (1-stats['ost'][t])\n",
    "\n",
    "        # 更新各权重矩阵的偏导数，由于所有时刻共享权值，故所有时刻累加\n",
    "        dwhf, dwxf, dbf = cal_grad_delta(dwhf, dwxf, dbf, delta_ft_net, stats['hst'][t-1], x[t])                              \n",
    "        dwhi, dwxi, dbi = cal_grad_delta(dwhi, dwxi, dbi, delta_it_net, stats['hst'][t-1], x[t])                              \n",
    "        dwha, dwxa, dba = cal_grad_delta(dwha, dwxa, dba, delta_at_net, stats['hst'][t-1], x[t])            \n",
    "        dwho, dwxo, dbo = cal_grad_delta(dwho, dwxo, dbo, delta_ot_net, stats['hst'][t-1], x[t])\n",
    "\n",
    "    return [dwhf, dwxf, dbf, \n",
    "            dwhi, dwxi, dbi, \n",
    "            dwha, dwxa, dba, \n",
    "            dwho, dwxo, dbo, \n",
    "            dwy, dby]\n",
    "    pass\n",
    "\n",
    "#计算权重偏导数的公共方法\n",
    "def cal_grad_delta(dwh, dwx, db, delta_net, ht_pre, x):\n",
    "    dwh += delta_net * ht_pre\n",
    "    dwx += delta_net * x\n",
    "    db += delta_net\n",
    "\n",
    "    return dwh, dwx, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#更新梯度,\n",
    "def sgd_step(x, y, learning_rate):\n",
    "    dwhf, dwxf, dbf, dwhi, dwxi, dbi, dwha, dwxa, dba, dwho, dwxo, dbo, dwy, dby = bptt(x, y)\n",
    "#     print(dwhf[0, 0])\n",
    "    \n",
    "    #更新权重矩阵\n",
    "    variables['whf'], variables['wxf'], variables['bf'] = update_wb(learning_rate, variables['whf'], variables['wxf'], variables['bf'], dwhf, dwxf, dbf)\n",
    "    variables['whi'], variables['wxi'], variables['bi'] = update_wb(learning_rate, variables['whi'], variables['wxi'], variables['bi'], dwhi, dwxi, dbi) \n",
    "    variables['wha'], variables['wxa'], variables['ba'] = update_wb(learning_rate, variables['wha'], variables['wxa'], variables['ba'], dwha, dwxa, dba) \n",
    "    variables['who'], variables['wxo'], variables['bo'] = update_wb(learning_rate, variables['who'], variables['wxo'], variables['bo'], dwho, dwxo, dbo) \n",
    "    variables['wy'] = variables['wy'] - learning_rate * dwy\n",
    "    variables['by'] = variables['by'] - learning_rate * dby\n",
    "    \n",
    "#更新权重矩阵的公共方法\n",
    "def update_wb(learning_rate, wh, wx, b, dwh, dwx, db):\n",
    "    wh -= learning_rate * dwh\n",
    "    wx -= learning_rate * dwx\n",
    "    b -= learning_rate * db\n",
    "    return wh, wx, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#训练lstm\n",
    "def train(x_train, y_train, learning_rate=0.005, n_epoch=5):\n",
    "    losses = []\n",
    "    num_examples = 0.05\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        for i in range(len(x_train)):\n",
    "            sgd_step(x_train[i], y_train[i], learning_rate)\n",
    "            num_examples += 1\n",
    "        \n",
    "        cost = loss(x_train, y_train)\n",
    "        losses.append(cost)\n",
    "        print(\"epoch:{},loss:{}\".format(epoch + 1, cost))\n",
    "        if len(losses) > 1 and losses[-1] > losses[-2]:\n",
    "            learning_rate *= 0.5\n",
    "            print(\"decrease learning_rate to {}\".format(learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1,loss:6.327918914206463\n",
      "epoch:2,loss:6.075115714313507\n"
     ]
    }
   ],
   "source": [
    "train(x_train[:200], y_train[:200], n_epoch=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
