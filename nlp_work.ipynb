{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    自然语言处理课程作业复现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import gc\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#文件位置\n",
    "data_path = \"nlp_data/\"\n",
    "train_cws = data_path + \"train_cws.txt\"\n",
    "train_ner = data_path + \"train_ner.txt\"\n",
    "train_pos = data_path + \"train_pos.txt\"\n",
    "\n",
    "test_cws = data_path + \"test_cws1.txt\"\n",
    "test_ner = data_path + \"test_ner1.txt\"\n",
    "test_pos = data_path + \"test_pos1.txt\"\n",
    "\n",
    "val_cws = data_path + \"val_cws.txt\"\n",
    "val_ner = data_path + \"val_ner.txt\"\n",
    "val_pos = data_path + \"val_pos.txt\"\n",
    "\n",
    "#预处理文件目录\n",
    "preprocess_path = data_path + 'preprocess/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(id2word):13368,max(id2word.keys):13367\n"
     ]
    }
   ],
   "source": [
    "#cws训练数据预处理\n",
    "#将分词训练文件中的词和词分成的单个字都存在一个列表中统计\n",
    "cws_tag_id_path = preprocess_path + 'cws_tag_id.json'\n",
    "\n",
    "def preprocess_cws():\n",
    "    #cws训练文件预处理，得到字符id映射文件和id字符映射数据word2id和id2word\n",
    "    words = []\n",
    "    with open(train_cws, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            words.extend(line.strip().split(\" \"))\n",
    "    chars = []\n",
    "    for word in words:\n",
    "        chars.extend(list(word))\n",
    "    word_counts = Counter([w for w in words if len(w) > 2] + chars).most_common()\n",
    "    gc.collect()\n",
    "    all_words = [word for word, count in word_counts]\n",
    "    word2id = {\"<PAD>\" : 0, \"<UNK>\" : 1}\n",
    "    word2id.update({word: index for index, word in enumerate(all_words)})\n",
    "    id2word = {id : _word for _word, id in word2id.items()}\n",
    "    print(\"len(id2word):{},max(id2word.keys):{}\".format(len(id2word), max(id2word.keys())))\n",
    "    cws_tag2id = {\"B\" : 0, \"I\" : 1, \"E\" : 2, \"S\" : 3}\n",
    "    cws_id2tag ={id:_tag for _tag, id in cws_tag2id.items()}\n",
    "    with open(cws_tag_id_path, 'w') as f:\n",
    "        json.dump([cws_tag2id, cws_id2tag], f, indent=4, ensure_ascii=False, sort_keys=True)\n",
    "        \n",
    "preprocess_cws()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#pos训练文件预处理\n",
    "pos_tag_id_path = preprocess_path + 'pos_tag_id.json'\n",
    "def preprocess_pos():\n",
    "    with open(train_pos, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        all_tags = []\n",
    "        #词性标注以/为分隔符\n",
    "        for line in lines:\n",
    "            for word_tag_str in line.strip().split(\" \"):\n",
    "                if word_tag_str and (\"/\" in word_tag_str):\n",
    "                    word_tag = word_tag_str.split('/')\n",
    "                    #标签中有混淆的元素比如</sub>，按/分隔会把sub>分为标签\n",
    "                    if word_tag[-1] and word_tag[-1].isalnum():\n",
    "                        all_tags.append(word_tag[-1])\n",
    "        #将所有的词性标注符记录下来\n",
    "        tag_count = Counter(all_tags).most_common()\n",
    "        pos_tag2id = {}\n",
    "        for position in \"BIES\":\n",
    "            for pos_tag, cnt in tag_count:\n",
    "                tag = \"{}-{}\".format(position, pos_tag)\n",
    "                pos_tag2id[tag] = len(pos_tag2id) + 1\n",
    "        for pos_tag, cnt in tag_count:\n",
    "            pos_tag2id[pos_tag] = len(pos_tag2id) + 1\n",
    "        pos_id2tag = {id : _tag for _tag, id in pos_tag2id.items()}\n",
    "        with open(pos_tag_id_path, 'w') as f:\n",
    "            json.dump([pos_tag2id, pos_id2tag], f, indent=4, ensure_ascii=False, sort_keys=True)\n",
    "    pass\n",
    "preprocess_pos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ner训练文件处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#构建模型\n",
    "class CWSModel:\n",
    "    def __init__(self, data_dim, hidden_dim):\n",
    "        self.data_dim = data_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def build(self):\n",
    "        pass\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
